<?xml version='1.0' encoding='utf-8'?>
<Metadata><Title>RWKV: Reinventing RNNs for the Transformer Era</Title><Reference>

FETA: A benchmark for few-sample task transfer in open-domain dialogue

AlonAlbalak


Yi-LinTuan


PegahJandaghi


ConnorPryor


LukeYoffe


DeepakRamachandran


LiseGetoor


JayPujara


WilliamYang


Wang



Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing
the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi

United Arab Emirates. Association for Computational Linguistics
2022



</Reference><Reference>

Layer normalization

JimmyLei Ba


JamieRyan Kiros


GeoffreyEHinton


2016


</Reference><Reference>

An empirical evaluation of generic convolutional and recurrent networks for sequence modeling

JZicoShaojie Bai


VladlenKolter


Koltun


2018


</Reference><Reference>

TweetEval: Unified benchmark and comparative evaluation for tweet classification

FrancescoBarbieri


JoseCamacho-Collados


LuisEspinosa Anke


LeonardoNeves

10.18653/v1/2020.findings-emnlp.148


Findings of the Association for Computational Linguistics: EMNLP 2020

Online. Association for Computational Linguistics
2020



</Reference><Reference>


IzBeltagy


MatthewEPeters


ArmanCohan

arXiv:2004.05150
Longformer: The long-document transformer

2020


</Reference><Reference>


StellaBiderman


HaileySchoelkopf


QuentinAnthony


HerbieBradley


O'Kyle


EricBrien


MohammadHallahan


ShivanshuAflah Khan


Purohit


EdwardUsvsn Sai Prashanth


Raff

arXiv:2304.01373
Pythia: A suite for analyzing large language models across training and scaling

2023


arXiv preprint
</Reference><Reference>

Piqa: Reasoning about physical commonsense in natural language

YonatanBisk


RowanZellers


LeRonan


JianfengBras


YejinGao


Choi



Thirty-Fourth AAAI Conference on Artificial Intelligence

2020


</Reference><Reference>

Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow

SidBlack


LeoGao


PhilWang


ConnorLeahy


StellaBiderman

10.5281/zenodo


2022. 2021
5297715


</Reference><Reference>

Quasi-recurrent neural networks

JamesBradbury


StephenMerity


CaimingXiong


RichardSocher


2017


In ICLR
</Reference><Reference>

Language models are few-shot learners

TomBrown


BenjaminMann


NickRyder


MelanieSubbiah


JaredDKaplan


PrafullaDhariwal


ArvindNeelakantan


PranavShyam


GirishSastry


AmandaAskell



Advances in neural information processing systems

33

2020


</Reference><Reference>

Scaling transformer to 1m tokens and beyond with rmt

AydarBulatov


YuriKuratov


MikhailSBurtsev


2023


</Reference><Reference>

Recurrent memory transformer

AydarBulatov


YuryKuratov


MikhailBurtsev



Advances in Neural Information Processing Systems

2022
35



</Reference><Reference>

Measuring massive multitask language understanding

DanHendrycks


CollinBurns


StevenBasart


AndyZou


MantasMazeika


DawnSong


JacobSteinhardt



International Conference on Learning Representations

2021


</Reference><Reference>

The vanishing gradient problem during learning recurrent neural nets and problem solutions

SeppHochreiter



International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems

6
02

1998


</Reference><Reference>

Long short-term memory

SeppHochreiter


JürgenSchmidhuber



Neural Computation

9
8

1997


</Reference><Reference>



JordanHoffmann


SebastianBorgeaud


ArthurMensch


ElenaBuchatskaya


TrevorCai


ElizaRutherford


DiegoDe Las


LisaAnneCasas


JohannesHendricks


AidanWelbl


TomClark


EricHennigan


KatieNoland


GeorgeMillican


BogdanVan Den Driessche


AureliaDamoc


SimonGuy


KarenOsindero


ErichSimonyan


JackWElsen


OriolRae


LaurentVinyals


Sifre


2022


Training compute-optimal large language models
</Reference><Reference>

LoRA: Low-rank adaptation of large language models

JEdward


PhillipHu


ZeyuanWallis


YuanzhiAllen-Zhu


SheanLi


LuWang


WeizhuWang


Chen



International Conference on Learning Representations

2022


</Reference><Reference>

Deep learning for time series classification: a review

HassanIsmail Fawaz


GermainForestier


JonathanWeber


LhassaneIdoumghar


Pierre-AlainMuller



Data mining and knowledge discovery

33
4

2019


</Reference><Reference>

Perceiver: General perception with iterative attention

AndrewJaegle


FelixGimeno


AndyBrock


OriolVinyals


AndrewZisserman


JoaoCarreira

PMLR


International conference on machine learning

2021



</Reference><Reference>

Mnnfast: A fast and scalable system architecture for memory-augmented neural networks

HanhwiJang


JoonsungKim


Jae-EonJo


JaewonLee


JangwooKim



Proceedings of the 46th International Symposium on Computer Architecture
the 46th International Symposium on Computer Architecture

2019



</Reference><Reference>

Crowdsourcing multiple choice science questions
10.18653/v1/W17-4413
Matt Gardner Johannes Welbl Nelson F. Liu

2017


</Reference><Reference>

Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension

MandarJoshi


EunsolChoi


DanielSWeld


LukeZettlemoyer



ACL

2017


</Reference><Reference>

Highly accurate protein structure prediction with alphafold

JohnJumper


RichardEvans


AlexanderPritzel


TimGreen


MichaelFigurnov


OlafRonneberger


KathrynTunyasuvunakool


RussBates


AugustinŽídek


AnnaPotapenko

10.1038/s41586-021-03819-2


Nature

596
7873

2021


</Reference><Reference>

Preventing gradient explosions in gated recurrent units

SekitoshiKanai


YasuhiroFujiwara


SotetsuIwamura



NIPS

2017


</Reference><Reference>


JaredKaplan


SamMccandlish


TomHenighan


TomBBrown


BenjaminChess


RewonChild


ScottGray


AlecRadford


JeffreyWu


DarioAmodei

arXiv:2001.08361
Scaling laws for neural language models

2020


arXiv preprint
</Reference><Reference>

Transformers are rnns: Fast autoregressive transformers with linear attention

AngelosKatharopoulos


ApoorvVyas

PMLR


International Conference on Machine Learning

2020



Nikolaos Pappas, and François Fleuret
</Reference><Reference>

Reformer: The efficient transformer

NikitaKitaev


LKaiser


AnselmLevskaya

ArXiv, abs/2001.04451

2020


</Reference><Reference>



JanKocoń


IgorCichecki


OliwierKaszyca


MateuszKochanek


DominikaSzydło


JoannaBaran


JulitaBielaniewicz


MarcinGruza


ArkadiuszJanz


KamilKanclerz


AnnaKocoń


BartłomiejKoptyra


WiktoriaMieleszczenko-Kowszewicz


PiotrMiłkowski


MarcinOleksy


MaciejPiasecki


ŁukaszRadliński


KonradWojtasik


StanisławWoźniak


PrzemysławKazienko


2023


Chatgpt: Jack of all trades, master of none
</Reference><Reference>

Multi-level sentiment analysis of polemo 2.0: Extended corpus of multi-domain consumer reviews

JanKocoń


PiotrMiłkowski


MonikaZaśko-Zielińska



Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)
the 23rd Conference on Computational Natural Language Learning (CoNLL)

2019



</Reference><Reference>

Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive lstms

PhongLe


WillemZuidema



Proceedings of the 1st Workshop on Representation Learning for NLP
the 1st Workshop on Representation Learning for NLP

2016



</Reference><Reference>

Simple recurrent units for highly parallelizable recurrence

TaoLei


YuZhang


SidaIWang


HuiDai


YoavArtzi

10.18653/v1/D18-1477


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, Belgium

Association for Computational Linguistics
2018



</Reference><Reference>



HanxiaoLiu


ZihangDai


DavidRSo


VQuoc


Le


2021


Pay attention to mlps
</Reference><Reference>

Linear unified nested attention

XuezheMa


XiangKong


SinongWang


ChuntingZhou


JonathanMay


HaoMa


LukeZettlemoyer



Advances in Neural Information Processing Systems

34

2021
Luna


</Reference><Reference>

Mega: Moving average equipped gated attention

XuezheMa


ChuntingZhou


XiangKong


JunxianHe


LiangkeGui


GrahamNeubig


JonathanMay


LukeZettlemoyer



ICLR

2023


</Reference><Reference>

Parallelizing linear recurrent neural nets over sequence length

EricMartin


ChrisCundy

ArXiv, abs/1709.04057

2017


</Reference><Reference>

Locating and editing factual associations in GPT

KevinMeng


DavidBau


AlexAndonian


YonatanBelinkov



Advances in Neural Information Processing Systems

36
2022


</Reference><Reference>

Can a suit of armor conduct electricity? a new dataset for open book question answering

TodorMihaylov


PeterClark


TusharKhot


AshishSabharwal



EMNLP

2018


</Reference><Reference>


JohnMiller


MoritzHardt

arXiv: Learning
Stable recurrent models

2018


</Reference><Reference>

A corpus and cloze evaluation for deeper understanding of commonsense stories

NasrinMostafazadeh


NathanaelChambers


XiaodongHe


DeviParikh


DhruvBatra


LucyVanderwende


PushmeetKohli


JamesAllen



Proceedings of the 2016 Conference of the North American Chapter
the 2016 Conference of the North American Chapter

Human Language Technologies
2016



</Reference><Reference>

Introducing chatgpt

Openai



2022


</Reference><Reference>


Openai

Gpt-4 technical report

2023


</Reference><Reference>

Resurrecting recurrent neural networks for long sequences

AntonioOrvieto


LSamuel


AlbertSmith


AnushanGu


CaglarFernando


RazvanGulcehre


SohamPascanu


De

arXiv:2303.06349

2023


arXiv preprint
</Reference><Reference>


LongOuyang


JeffWu


XuJiang


DiogoAlmeida


CarrollLWainwright


PamelaMishkin


ChongZhang


SandhiniAgarwal


KatarinaSlama


AlexRay


JohnSchulman


JacobHilton


FraserKelton


LukeMiller


MaddieSimens


AmandaAskell


PeterWelinder


PaulChristiano


JanLeike


RyanLowe

Training language models to follow instructions with human feedback

2022


</Reference><Reference>

The LAMBADA dataset: Word prediction requiring a broad discourse context

DenisPaperno


GermánKruszewski


AngelikiLazaridou


NgocQuanPham


RaffaellaBernardi


SandroPezzelle


MarcoBaroni


GemmaBoleda


RaquelFernandez



Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany

Association for Computational Linguistics
2016
1



Long Papers)
</Reference><Reference>

On the difficulty of training recurrent neural networks

RazvanPascanu


TomasMikolov


YoshuaBengio



International Conference on Machine Learning

2012


</Reference><Reference>



AdamPaszke


SamGross


FranciscoMassa


AdamLerer


JamesBradbury


GregoryChanan


TrevorKilleen


ZemingLin


NataliaGimelshein


LucaAntiga


AlbanDesmaison


AndreasKöpf


EdwardYang


ZachDevito


MartinRaison


AlykhanTejani


SasankChilamkurthy


BenoitSteiner


LuFang


JunjieBai


SoumithChintala


2019


Pytorch: An imperative style, high-performance deep learning library
</Reference><Reference>


MichaelPoli


StefanoMassaroli


EricNguyen


DanielYFu


TriDao


StephenBaccus


YoshuaBengio


StefanoErmon


ChristopherRé

arXiv:2302.10866
Hyena hierarchy: Towards larger convolutional language models

2023


arXiv preprint
</Reference><Reference>

Train short, test long: Attention with linear biases enables input length extrapolation

OfirPress


NoahASmith


MikeLewis



The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event

2022. April 25-29, 2022


</Reference><Reference>

Six attributes of unhealthy conversations

IlanPrice


JordanGifford-Moore


JoryFlemming


SaulMusker


MaayanRoichman


GuillaumeSylvain


NithumThain


LucasDixon


JeffreySorensen

10.18653/v1/2020.alw-1.15


Proceedings of the Fourth Workshop on Online Abuse and Harms
the Fourth Workshop on Online Abuse and Harms

Online. Association for Computational Linguistics
2020



</Reference><Reference>

Selfattention does not need o(n 2 ) memory

MarkusNRabe


CharlesStaats


2022


</Reference><Reference>

Choice of plausible alternatives: An evaluation of commonsense causal reasoning

MelissaRoemmele


AndrewSCosmin Adrian Bejan


Gordon



AAAI

2018


</Reference><Reference>


LeTeven


AngelaScao


ChristopherFan


EllieAkiki


SuzanaPavlick


DanielIlić


RomanHesslow


AlexandraCastagné


FrançoisSasha Luccioni


MatthiasYvon


Gallé

arXiv:2211.05100
Bloom: A 176bparameter open-access multilingual language model

2022


arXiv preprint
</Reference><Reference>

SARCASMANIA: Sarcasm Exposed

RamshaSiddiqui



2019. February-2023
2


</Reference><Reference>

Primer: Searching for efficient transformers for language modeling

DavidRSo


WojciechManke


HanxiaoLiu


ZihangDai


NoamShazeer


VQuoc


Le

CoRR, abs/2109.08668

2021


</Reference><Reference>

Synthesizer: Rethinking self-attention in transformer models

YiTay


DaraBahri


DonaldMetzler


Da-ChengJuan


ZheZhao


CheZheng


2020


</Reference><Reference>

Efficient transformers: A survey

YiTay


MostafaDehghani


DaraBahri


DonaldMetzler



ACM Computing Surveys

55
6

2022


</Reference><Reference>

Mlp-mixer: An all-mlp architecture for vision

OIlya


NeilTolstikhin


AlexanderHoulsby


LucasKolesnikov


XiaohuaBeyer


ThomasZhai


JessicaUnterthiner


AndreasYung


DanielSteiner


JakobKeysers


MarioUszkoreit


AlexeyLucic


Dosovitskiy

CoRR, abs/2105.01601

2021


</Reference><Reference>

Llama: Open and efficient foundation language models. Aäron van den Oord

HugoTouvron


ThibautLavril


GautierIzacard


XavierMartinet


Marie-AnneLachaux


TimothéeLacroix


BaptisteRozière


NamanGoyal


EricHambro


FaisalAzhar


AurelienRodriguez


ArmandJoulin


EdouardGrave


GuillaumeLample

ArXiv, abs/1609.03499
Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu

2023. 2016


Wavenet: A generative model for raw audio
</Reference><Reference>

Attention is all you need

AshishVaswani


NoamShazeer


NikiParmar


JakobUszkoreit


LlionJones


AidanNGomez


ŁukaszKaiser


IlliaPolosukhin



Advances in Neural Information Processing Systems

Curran Associates, Inc
2017
30


</Reference><Reference>

Head-qa: A healthcare dataset for complex reasoning

DavidVilares


CarlosGómez-Rodríguez



ACL

2019


</Reference><Reference>

Superglue: A stickier benchmark for general-purpose language understanding systems

AlexWang


YadaPruksachatkun


NikitaNangia


AmanpreetSingh


JulianMichael


FelixHill


OmerLevy


SamuelBowman



Advances in Neural Information Processing Systems

Curran Associates, Inc
2019
32


</Reference><Reference>

GLUE: A multi-task benchmark and analysis platform for natural language understanding

AlexWang


AmanpreetSingh


JulianMichael


FelixHill


OmerLevy


SamuelBowman

10.18653/v1/W18-5446


Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP
the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, Belgium

Association for Computational Linguistics
2018



</Reference><Reference>

Linformer: Self-attention with linear complexity

SinongWang


BelindaZLi


MadianKhabsa


HanFang


HaoMa


2020


</Reference><Reference>

A comprehensive survey on graph neural networks

ZonghanWu


ShiruiPan


FengwenChen


GuodongLong


ChengqiZhang


SYu


Philip



IEEE transactions on neural networks and learning systems

2020
32



</Reference><Reference>

Ex machina: Personal attacks seen at scale

ElleryWulczyn


NithumThain


LucasDixon

10.1145/3038912.3052591


Proceedings of the 26th International Conference on World Wide Web
the 26th International Conference on World Wide WebPerth, Australia

ACM
2017. 2017. April 3-7, 2017



</Reference><Reference>

Metaformer is actually what you need for vision

WeihaoYu


MiLuo


PanZhou


ChenyangSi


YichenZhou


XinchaoWang


JiashiFeng


ShuichengYan


2022


</Reference><Reference>

Big bird: Transformers for longer sequences

ManzilZaheer


GuruGuruganesh


AvinavaKumar


JoshuaDubey


ChrisAinslie


SantiagoAlberti


PhilipOntanon


AnirudhPham


QifanRavula


LiWang


Yang



Advances in Neural Information Processing Systems

2020
33


</Reference><Reference>

Hellaswag: Can a machine really finish your sentence

RowanZellers


AriHoltzman


YonatanBisk


AliFarhadi


YejinChoi



ACL

2019


</Reference><Reference>

Winogrande: An adversarial winograd schema challenge at scale

RowanZellers


AriHoltzman


YonatanBisk


AliFarhadi


YejinChoi



ACL

2020


</Reference><Reference>



ShuangfeiZhai


WalterTalbott


NitishSrivastava


ChenHuang


HanlinGoh


RuixiangZhang


JoshSusskind


2021


An attention free transformer
</Reference><Reference>

Record: Bridging the gap between human and machine commonsense reading comprehension

ShengZhang


XiaodongLiu


JingjingLiu


JianfengGao


KevinDuh


BenjaminVan Durme

arXiv:1810.12885

2018


</Reference><Reference>


SusanZhang


StephenRoller


NamanGoyal


MikelArtetxe


MoyaChen


ShuohuiChen


ChristopherDewan


MonaDiab


XianLi


XiVictoria Lin

arXiv:2205.01068
Opt: Open pre-trained transformer language models

2022


arXiv preprint
</Reference><Reference>

Alon Albalak Manuscript


abstract and sections 1, 9; proofreading and revision
</Reference><Reference>

Contributions Appendix J. Ruichong Zhang Manuscript (proofreading and revision); Contributions to Figure 5 and Appendix K

JanKocon


Manuscript



Stanisław Woźniak Appendix J. Bartłomiej Koptyra Contributions to Appendix J. RWKV-4


Contributions to Appendix J. Przemysław Kazienko Manuscript (section 6; proofreading and revision)
</Reference><Reference>



Gpt-Neox





</Reference></Metadata>